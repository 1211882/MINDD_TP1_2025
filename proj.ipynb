{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c622a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas: 41176\n",
      "\n",
      "Percentagem de 'unknown' por coluna:\n",
      "age                0.000000\n",
      "job                0.801438\n",
      "marital            0.194288\n",
      "education          4.201477\n",
      "default           20.876239\n",
      "housing            2.404313\n",
      "loan               2.404313\n",
      "contact            0.000000\n",
      "month              0.000000\n",
      "day_of_week        0.000000\n",
      "duration           0.000000\n",
      "campaign           0.000000\n",
      "pdays              0.000000\n",
      "previous           0.000000\n",
      "poutcome           0.000000\n",
      "emp.var.rate       0.000000\n",
      "cons.price.idx     0.000000\n",
      "cons.conf.idx      0.000000\n",
      "euribor3m          0.000000\n",
      "nr.employed        0.000000\n",
      "y                  0.000000\n",
      "dtype: float64\n",
      "\n",
      "Percentagem de 'nonexistent' em 'poutcome': 86.34%\n",
      "\n",
      "Percentagem de '0' em 'duration': 0.01%\n",
      "\n",
      "Percentagem de '999' em 'pdays': 96.32%\n",
      "\n",
      "Distribuição da variável alvo (y):\n",
      "y\n",
      "no     88.733728\n",
      "yes    11.266272\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Valores nulos por coluna:\n",
      "age                              0\n",
      "campaign                         0\n",
      "previous                         0\n",
      "emp.var.rate                     0\n",
      "cons.price.idx                   0\n",
      "cons.conf.idx                    0\n",
      "euribor3m                        0\n",
      "nr.employed                      0\n",
      "y                                0\n",
      "month_ordinal                    0\n",
      "month_apr                        0\n",
      "month_aug                        0\n",
      "month_dec                        0\n",
      "month_jul                        0\n",
      "month_jun                        0\n",
      "month_mar                        0\n",
      "month_may                        0\n",
      "month_nov                        0\n",
      "month_oct                        0\n",
      "month_sep                        0\n",
      "day_fri                          0\n",
      "day_mon                          0\n",
      "day_thu                          0\n",
      "day_tue                          0\n",
      "day_wed                          0\n",
      "job_blue-collar                  0\n",
      "job_entrepreneur                 0\n",
      "job_housemaid                    0\n",
      "job_management                   0\n",
      "job_retired                      0\n",
      "job_self-employed                0\n",
      "job_services                     0\n",
      "job_student                      0\n",
      "job_technician                   0\n",
      "job_unemployed                   0\n",
      "marital_married                  0\n",
      "marital_single                   0\n",
      "education_basic.6y               0\n",
      "education_basic.9y               0\n",
      "education_high.school            0\n",
      "education_illiterate             0\n",
      "education_professional.course    0\n",
      "education_university.degree      0\n",
      "default_unknown                  0\n",
      "default_yes                      0\n",
      "housing_yes                      0\n",
      "loan_yes                         0\n",
      "contact_telephone                0\n",
      "poutcome_nonexistent             0\n",
      "poutcome_success                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ler o CSV\n",
    "df = pd.read_csv(\"bank_marketing/bank.csv\", sep=\";\")\n",
    "\n",
    "# eliminar duplicados\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Mostrar número de linhas\n",
    "print(f\"Número de linhas: {len(df)}\")\n",
    "\n",
    "# Normalizar nomes das colunas\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Contagem de 'unknown' em percentagem\n",
    "percentagem_nulos = (df == \"unknown\").sum() * 100 / len(df)\n",
    "print(\"\\nPercentagem de 'unknown' por coluna:\")\n",
    "print(percentagem_nulos)\n",
    "\n",
    "#modalizar unknown \n",
    "df['marital'] = df['marital'].replace('unknown', df['marital'].mode()[0])\n",
    "df['education'] = df['education'].replace('unknown', df['education'].mode()[0])\n",
    "df['housing'] = df['housing'].replace('unknown', df['housing'].mode()[0])\n",
    "df['loan'] = df['loan'].replace('unknown', df['loan'].mode()[0])\n",
    "df['job'] = df['job'].replace('unknown', df['job'].mode()[0])\n",
    "\n",
    "#   converter meses para valores ordinais\n",
    "month_mapping = {\"jan\":1, \"feb\":2, \"mar\":3, \"apr\":4, \"may\":5, \"jun\":6,\n",
    "                 \"jul\":7, \"aug\":8, \"sep\":9, \"oct\":10, \"nov\":11, \"dec\":12}\n",
    "\n",
    "df[\"month_ordinal\"] = df[\"month\"].map(month_mapping)\n",
    "\n",
    "#one hot encoding para meses\n",
    "df_month = pd.get_dummies(df[\"month\"], prefix=\"month\")\n",
    "df = pd.concat([df, df_month], axis=1)\n",
    "df.drop(\"month\", axis=1, inplace=True)\n",
    "\n",
    "#converter dias da semana para valores ordinais\n",
    "df_day = pd.get_dummies(df[\"day_of_week\"], prefix=\"day\")\n",
    "df = pd.concat([df, df_day], axis=1)\n",
    "df.drop(\"day_of_week\", axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#contagem de nonexists\n",
    "contagem_nonexists = (df[\"poutcome\"] == \"nonexistent\").mean() * 100\n",
    "print(f\"\\nPercentagem de 'nonexistent' em 'poutcome': {contagem_nonexists:.2f}%\")\n",
    "\n",
    "#contagem de 0 em previous\n",
    "contagem_0 = (df[\"duration\"] == 0).mean() * 100\n",
    "print(f\"\\nPercentagem de '0' em 'duration': {contagem_0:.2f}%\")\n",
    "\n",
    "#eliminar coluna duration\n",
    "df = df.drop(columns=[\"duration\"])\n",
    "\n",
    "\n",
    "#contagem de valores 999 em pdays\n",
    "contagem_999 = (df[\"pdays\"] == 999).mean() * 100\n",
    "print(f\"\\nPercentagem de '999' em 'pdays': {contagem_999:.2f}%\")\n",
    "\n",
    "#eliminar coluna pdays\n",
    "df = df.drop(columns=[\"pdays\"]) \n",
    "\n",
    "# Distribuição da variável alvo (em percentagem)\n",
    "print(\"\\nDistribuição da variável alvo (y):\")   \n",
    "print(df[\"y\"].value_counts(normalize=True) * 100)\n",
    "\n",
    "#separar variaveis independentes do alvo\n",
    "X = df.drop(columns=[\"y\"])\n",
    "y = df[\"y\"] \n",
    "\n",
    "#importar min e max scaler \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df[['age', 'campaign', 'previous']] = scaler.fit_transform(df[['age', 'campaign', 'previous']])\n",
    "\n",
    "#padronizar colunas categoricas\n",
    "df = pd.get_dummies(df, columns=['job', 'marital', 'education', 'default', \n",
    "                                 'housing', 'loan', 'contact', 'poutcome'], drop_first=True)\n",
    "\n",
    "#verificar se existe valores nulos  \n",
    "print(\"\\nValores nulos por coluna:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b634f8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas após duplicados: 41176\n",
      "\n",
      "Percentagem de 'unknown' por coluna:\n",
      "age                0.000000\n",
      "job                0.801438\n",
      "marital            0.194288\n",
      "education          4.201477\n",
      "default           20.876239\n",
      "housing            2.404313\n",
      "loan               2.404313\n",
      "contact            0.000000\n",
      "month              0.000000\n",
      "day_of_week        0.000000\n",
      "duration           0.000000\n",
      "campaign           0.000000\n",
      "pdays              0.000000\n",
      "previous           0.000000\n",
      "poutcome           0.000000\n",
      "emp.var.rate       0.000000\n",
      "cons.price.idx     0.000000\n",
      "cons.conf.idx      0.000000\n",
      "euribor3m          0.000000\n",
      "nr.employed        0.000000\n",
      "y                  0.000000\n",
      "dtype: float64\n",
      "\n",
      "Percentagem de '0' em 'duration': 0.01%\n",
      "\n",
      "Percentagem de '999' em 'pdays': 96.32%\n",
      "\n",
      "Distribuição da variável alvo (y):\n",
      "y\n",
      "0    88.733728\n",
      "1    11.266272\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Valores nulos por coluna:\n",
      "age                              0\n",
      "campaign                         0\n",
      "previous                         0\n",
      "emp.var.rate                     0\n",
      "cons.price.idx                   0\n",
      "cons.conf.idx                    0\n",
      "euribor3m                        0\n",
      "nr.employed                      0\n",
      "y                                0\n",
      "month_ordinal                    0\n",
      "day_ordinal                      0\n",
      "job_blue-collar                  0\n",
      "job_entrepreneur                 0\n",
      "job_housemaid                    0\n",
      "job_management                   0\n",
      "job_retired                      0\n",
      "job_self-employed                0\n",
      "job_services                     0\n",
      "job_student                      0\n",
      "job_technician                   0\n",
      "job_unemployed                   0\n",
      "marital_married                  0\n",
      "marital_single                   0\n",
      "education_basic.6y               0\n",
      "education_basic.9y               0\n",
      "education_high.school            0\n",
      "education_illiterate             0\n",
      "education_professional.course    0\n",
      "education_university.degree      0\n",
      "default_unknown                  0\n",
      "default_yes                      0\n",
      "housing_yes                      0\n",
      "loan_yes                         0\n",
      "contact_telephone                0\n",
      "poutcome_nonexistent             0\n",
      "poutcome_success                 0\n",
      "dtype: int64\n",
      "\n",
      "Colunas finais do DataFrame:\n",
      "Index(['age', 'campaign', 'previous', 'emp.var.rate', 'cons.price.idx',\n",
      "       'cons.conf.idx', 'euribor3m', 'nr.employed', 'y', 'month_ordinal',\n",
      "       'day_ordinal', 'job_blue-collar', 'job_entrepreneur', 'job_housemaid',\n",
      "       'job_management', 'job_retired', 'job_self-employed', 'job_services',\n",
      "       'job_student', 'job_technician', 'job_unemployed', 'marital_married',\n",
      "       'marital_single', 'education_basic.6y', 'education_basic.9y',\n",
      "       'education_high.school', 'education_illiterate',\n",
      "       'education_professional.course', 'education_university.degree',\n",
      "       'default_unknown', 'default_yes', 'housing_yes', 'loan_yes',\n",
      "       'contact_telephone', 'poutcome_nonexistent', 'poutcome_success'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#versão chatGPT\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Ler CSV e remover duplicados\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\"bank_marketing/bank.csv\", sep=\";\")\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Normalizar nomes das colunas\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "print(f\"Número de linhas após duplicados: {len(df)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Verificação e tratamento de 'unknown'\n",
    "# -----------------------------\n",
    "# Percentagem de 'unknown' por coluna\n",
    "percentagem_nulos = (df == \"unknown\").sum() * 100 / len(df)\n",
    "print(\"\\nPercentagem de 'unknown' por coluna:\")\n",
    "print(percentagem_nulos)\n",
    "\n",
    "# Substituir 'unknown' pela moda das colunas categóricas\n",
    "for col in ['marital', 'education', 'housing', 'loan', 'job']:\n",
    "    df[col] = df[col].replace('unknown', df[col].mode()[0])\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Transformação de variáveis categóricas\n",
    "# -----------------------------\n",
    "\n",
    "# Variáveis ordinal ou one-hot: mês e dia da semana\n",
    "month_mapping = {\"jan\":1, \"feb\":2, \"mar\":3, \"apr\":4, \"may\":5, \"jun\":6,\n",
    "                 \"jul\":7, \"aug\":8, \"sep\":9, \"oct\":10, \"nov\":11, \"dec\":12}\n",
    "df['month_ordinal'] = df['month'].map(month_mapping)\n",
    "\n",
    "day_mapping = {\"mon\":1, \"tue\":2, \"wed\":3, \"thu\":4, \"fri\":5}\n",
    "df['day_ordinal'] = df['day_of_week'].map(day_mapping)\n",
    "\n",
    "# Outras variáveis categóricas → one-hot encoding\n",
    "categorical_cols = ['job', 'marital', 'education', 'default', \n",
    "                    'housing', 'loan', 'contact', 'poutcome']\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Remover colunas originais já processadas\n",
    "df.drop(columns=['month', 'day_of_week'], inplace=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ Tratar colunas irrelevantes\n",
    "# -----------------------------\n",
    "# Percentagem de 0 em duration\n",
    "duration_zero_pct = (df['duration'] == 0).mean() * 100\n",
    "print(f\"\\nPercentagem de '0' em 'duration': {duration_zero_pct:.2f}%\")\n",
    "df.drop(columns=['duration'], inplace=True)\n",
    "\n",
    "# Percentagem de 999 em pdays\n",
    "pdays_999_pct = (df['pdays'] == 999).mean() * 100\n",
    "print(f\"\\nPercentagem de '999' em 'pdays': {pdays_999_pct:.2f}%\")\n",
    "df.drop(columns=['pdays'], inplace=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Variável alvo\n",
    "# -----------------------------\n",
    "df['y'] = df['y'].map({'no': 0, 'yes': 1})\n",
    "print(\"\\nDistribuição da variável alvo (y):\")\n",
    "print(df['y'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ Normalizar colunas numéricas\n",
    "# -----------------------------\n",
    "scaler = MinMaxScaler()\n",
    "numeric_cols = ['age', 'campaign', 'previous']\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "# -----------------------------\n",
    "# 7️⃣ Verificação final\n",
    "# -----------------------------\n",
    "print(\"\\nValores nulos por coluna:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nColunas finais do DataFrame:\")\n",
    "print(df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ea4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afc61a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aoazevedo\\AppData\\Local\\Temp\\ipykernel_22764\\678263784.py:9: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_small = df.groupby('y', group_keys=False).apply(sample_group).reset_index(drop=True)\n",
      "C:\\Users\\Aoazevedo\\AppData\\Local\\Temp\\ipykernel_22764\\678263784.py:13: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['day_of_week'] = pd.to_datetime(df['day_of_week'])\n"
     ]
    },
    {
     "ename": "DateParseError",
     "evalue": "Unable to parse datetime string: mon, at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDateParseError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      9\u001b[39m df_small = df.groupby(\u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m, group_keys=\u001b[38;5;28;01mFalse\u001b[39;00m).apply(sample_group).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#Split temporal (treino/val/test por datas)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mday_of_week\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mday_of_week\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Define limites (ajusta as datas conforme os teus dados)\u001b[39;00m\n\u001b[32m     16\u001b[39m train_end = \u001b[33m'\u001b[39m\u001b[33m2011-12-31\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\tools\\datetimes.py:1068\u001b[39m, in \u001b[36mto_datetime\u001b[39m\u001b[34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[39m\n\u001b[32m   1066\u001b[39m             result = arg.tz_localize(\u001b[33m\"\u001b[39m\u001b[33mutc\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1067\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[32m-> \u001b[39m\u001b[32m1068\u001b[39m     cache_array = \u001b[43m_maybe_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_listlike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1069\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array.empty:\n\u001b[32m   1070\u001b[39m         result = arg.map(cache_array)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\tools\\datetimes.py:249\u001b[39m, in \u001b[36m_maybe_cache\u001b[39m\u001b[34m(arg, format, cache, convert_listlike)\u001b[39m\n\u001b[32m    247\u001b[39m unique_dates = unique(arg)\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) < \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     cache_dates = \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\tools\\datetimes.py:437\u001b[39m, in \u001b[36m_convert_listlike_datetimes\u001b[39m\u001b[34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33mmixed\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m result, tz_parsed = \u001b[43mobjects_to_datetime64\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_object\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    447\u001b[39m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[32m    448\u001b[39m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n\u001b[32m    449\u001b[39m     out_unit = np.datetime_data(result.dtype)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\arrays\\datetimes.py:2415\u001b[39m, in \u001b[36mobjects_to_datetime64\u001b[39m\u001b[34m(data, dayfirst, yearfirst, utc, errors, allow_object, out_unit)\u001b[39m\n\u001b[32m   2412\u001b[39m \u001b[38;5;66;03m# if str-dtype, convert\u001b[39;00m\n\u001b[32m   2413\u001b[39m data = np.asarray(data, dtype=np.object_)\n\u001b[32m-> \u001b[39m\u001b[32m2415\u001b[39m result, tz_parsed = \u001b[43mtslib\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray_to_datetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2416\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2417\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2420\u001b[39m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreso\u001b[49m\u001b[43m=\u001b[49m\u001b[43mabbrev_to_npy_unit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_unit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2422\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2425\u001b[39m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[32m   2426\u001b[39m     \u001b[38;5;66;03m#  is in UTC\u001b[39;00m\n\u001b[32m   2427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result, tz_parsed\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslib.pyx:412\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslib.pyx:596\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslib.pyx:553\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/conversion.pyx:641\u001b[39m, in \u001b[36mpandas._libs.tslibs.conversion.convert_str_to_tsobject\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/parsing.pyx:336\u001b[39m, in \u001b[36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/parsing.pyx:678\u001b[39m, in \u001b[36mpandas._libs.tslibs.parsing.dateutil_parse\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mDateParseError\u001b[39m: Unable to parse datetime string: mon, at position 0"
     ]
    }
   ],
   "source": [
    "#Amostra exploratória estratificada\n",
    "# df tem colunas: 'customer_id', 'date', 'y'\n",
    "sample_frac = 0.05\n",
    "\n",
    "def sample_group(x):\n",
    "    n = max(1, int(round(len(x) * sample_frac)))  # pelo menos 1 por grupo\n",
    "    return x.sample(n=n, random_state=42)\n",
    "\n",
    "df_small = df.groupby('y', group_keys=False).apply(sample_group).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#Split temporal (treino/val/test por datas)\n",
    "df['day_of_week'] = pd.to_datetime(df['day_of_week'])\n",
    "\n",
    "# Define limites (ajusta as datas conforme os teus dados)\n",
    "train_end = '2011-12-31'\n",
    "val_end   = '2012-12-31'\n",
    "\n",
    "train = df[df['day_of_week'] <= train_end].copy()\n",
    "val   = df[(df['day_of_week'] > train_end) & (df['day_of_week'] <= val_end)].copy()\n",
    "test  = df[df['day_of_week'] > val_end].copy()\n",
    "\n",
    "print(len(train), len(val), len(test))\n",
    "\n",
    "# oversampling\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train = train.drop(columns=['y','customer_id','date'])\n",
    "y_train = train['y']\n",
    "\n",
    "# Pipeline exemplo\n",
    "pipe = ImbPipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "# GroupKFold por customer_id (para CV que evita vazamento entre clientes)\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score\n",
    "\n",
    "X = train.drop(columns=['y','date'])\n",
    "y = train['y']\n",
    "groups = train['customer_id']\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y, groups=groups)):\n",
    "    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "    y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "    # aplica oversampling apenas em X_tr/y_tr se quiser\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def precision_at_k(y_true, y_score, k=0.05):\n",
    "    # k como fracção do dataset (ex. 0.05 = top 5%)\n",
    "    n = int(len(y_score) * k)\n",
    "    idx = np.argsort(y_score)[::-1][:n]\n",
    "    return y_true.iloc[idx].mean()\n",
    "\n",
    "# após treinar o modelo e obter scores no test:\n",
    "y_scores = model.predict_proba(X_test)[:,1]\n",
    "print(\"Precision@5%:\", precision_at_k(y_test.reset_index(drop=True), pd.Series(y_scores), k=0.05))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a66aa0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41185\n"
     ]
    }
   ],
   "source": [
    "#eliminar quem já falhou pagamentos\n",
    "df.drop(df[df[\"default\"] == \"yes\"].index, inplace=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c08a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "41139\n",
      "35534\n",
      "age                  0\n",
      "job                271\n",
      "marital             54\n",
      "education         1453\n",
      "default           8018\n",
      "housing            851\n",
      "loan               851\n",
      "contact              0\n",
      "month                0\n",
      "day_of_week          0\n",
      "duration             0\n",
      "campaign             0\n",
      "pdays                0\n",
      "previous             0\n",
      "poutcome             0\n",
      "emp.var.rate         0\n",
      "cons.price.idx       0\n",
      "cons.conf.idx        0\n",
      "euribor3m            0\n",
      "nr.employed          0\n",
      "y                    0\n",
      "dtype: int64\n",
      "404\n",
      "2483\n",
      "678\n",
      "473\n",
      "1605\n",
      "2516\n"
     ]
    }
   ],
   "source": [
    "linhas_filtradas = df[((df[\"job\"] == \"unknown\") | (df[\"marital\"] == \"unknown\")) & (df[\"y\"] == \"yes\")]\n",
    "print(len(linhas_filtradas))\n",
    "#eliminar quem tem job ou marital unknown e y yes\n",
    "df = df[~(((df[\"job\"] == \"unknown\") | (df[\"marital\"] == \"unknown\")) & (df[\"y\"] == \"yes\"))]\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "linhas_filtradas = df[(df[\"previous\"] == 0) ]\n",
    "print(len(linhas_filtradas))\n",
    "\n",
    "linhas_filtradas = df[(df[\"housing\"] == \"yes\") & (df[\"loan\"] == \"yes\") & (df[\"y\"] == \"yes\")]\n",
    "print(len(linhas_filtradas))\n",
    "linhas_filtradas = df[(df[\"housing\"] == \"yes\") &  (df[\"y\"] == \"yes\")]\n",
    "print(len(linhas_filtradas))\n",
    "linhas_filtradas = df[ (df[\"loan\"] == \"yes\") & (df[\"y\"] == \"yes\")]\n",
    "print(len(linhas_filtradas))\n",
    "linhas_filtradas = df[ (df[\"marital\"] == \"divorced\")  & (df[\"y\"] == \"yes\")]\n",
    "print(len(linhas_filtradas))\n",
    "linhas_filtradas = df[ (df[\"marital\"] == \"single\")  & (df[\"y\"] == \"yes\")]\n",
    "print(len(linhas_filtradas))  \n",
    "linhas_filtradas = df[ (df[\"marital\"] == \"married\")  & (df[\"y\"] == \"yes\")]\n",
    "print(len(linhas_filtradas))  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "17d1a3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan\n",
      "no         33947\n",
      "yes         6248\n",
      "unknown      990\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(count_loan) \n\u001b[32m      4\u001b[39m df_loan = df.drop(df[df[\u001b[33m\"\u001b[39m\u001b[33mloan\u001b[39m\u001b[33m\"\u001b[39m] != \u001b[33m\"\u001b[39m\u001b[33myes\u001b[39m\u001b[33m\"\u001b[39m].index, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m n =\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf_loan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m df_loan.drop(df_loan[df_loan[\u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33myes\u001b[39m\u001b[33m\"\u001b[39m].index, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     10\u001b[39m nlen(df_loan)\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "count_loan = df[\"loan\"].value_counts()\n",
    "print(count_loan) \n",
    "\n",
    "df_loan = df.drop(df[df[\"loan\"] != \"yes\"].index, inplace=True)\n",
    "\n",
    "n =len(df_loan)\n",
    " \n",
    "\n",
    "df_loan.drop(df_loan[df_loan[\"default\"] == \"yes\"].index, inplace=True)\n",
    "nlen(df_loan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e469ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:n//3]\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
