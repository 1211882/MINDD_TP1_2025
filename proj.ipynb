{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f21f50e",
   "metadata": {},
   "source": [
    "# 1. Data Presentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e731061",
   "metadata": {},
   "source": [
    "### Import essential libraries:\n",
    "\n",
    "- for data manipulation and visualization: Numpy, Pandas, Seaborn, matplotlib\n",
    "\n",
    "- for preprocessing and handling data imbalance: sklearn.preprocessing, sklearn.impute, imblearn.under_sampling\n",
    "\n",
    "- for text processing: sklearn.feature_extraction.text\n",
    "\n",
    "- for model building and evaluation: sklearn.model_selection, sklearn.metrics\n",
    "\n",
    "- Machine learning models: sklearn.linear_model, sklearn.esemble, sklearn.svm, sklearn.neighbors, sklearn.naive_bayes\n",
    "\n",
    "- Gradient boosting models: xgboost, lightgbm\n",
    "\n",
    "- for building pipelines and transformations: sklearn.pipeline, sklearn.compose\n",
    "\n",
    "- for statistical tests: scipy.stats\n",
    "\n",
    "- Miscellaneous: warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f313053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Data splitting and evaluation\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedKFold, cross_validate, RandomizedSearchCV, cross_val_score\n",
    ")\n",
    "from scipy.stats import chi2_contingency, f_oneway\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, f1_score,\n",
    "    balanced_accuracy_score, matthews_corrcoef,\n",
    "    classification_report, confusion_matrix,\n",
    "    RocCurveDisplay, PrecisionRecallDisplay, make_scorer, \n",
    "    ConfusionMatrixDisplay, classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "# Models \n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    "    StackingClassifier\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import RFE, SelectFromModel\n",
    "\n",
    "# class imbalance & optimization\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from scipy.stats import randint, uniform, chi2_contingency\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a81ad9",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be409b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"bank_marketing/bank.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc9de05",
   "metadata": {},
   "source": [
    "### First 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e48cca3",
   "metadata": {},
   "source": [
    "### Last 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb4d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c08d561",
   "metadata": {},
   "source": [
    "### Checking how many columns (features + target) does the set contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f7ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa52b180",
   "metadata": {},
   "source": [
    "### Checking what columns the dataset has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340809bc",
   "metadata": {},
   "source": [
    "### Checking the type of the colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa92332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "num_cols = [c for c in df.columns if c not in cat_cols]\n",
    "\n",
    "print('\\nCategorical columns:', cat_cols)\n",
    "print('Numerical columns:', num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc671f6",
   "metadata": {},
   "source": [
    "### Checking the shape (rows, columns) in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2487776",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c184dec3",
   "metadata": {},
   "source": [
    "### Checking the size (number of cells) in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfe8186",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a04112d",
   "metadata": {},
   "source": [
    "### An overview of the data types, missing values, percentage of missing values, and the number of unique values for each column in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a204237",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes(exclude='object').columns\n",
    "\n",
    "data_info_num = pd.DataFrame({\n",
    "    'Data Type': df[num_cols].dtypes,\n",
    "    'Missing Values': df[num_cols].isnull().sum(),\n",
    "    'Percentage Missing': (df[num_cols].isnull().sum() / len(df)) * 100,\n",
    "    'Unique Values': df[num_cols].nunique()\n",
    "})\n",
    "\n",
    "data_info_num = data_info_num.sort_values(by='Missing Values', ascending=False)\n",
    "data_info_num.style.format({'Percentage Missing': '{:.2f}%'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f39155",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes(include='object').columns\n",
    "\n",
    "cat_info = pd.DataFrame({\n",
    "    'Data Type': df[cat_cols].dtypes,\n",
    "    'Unknown Count': [(df[c] == 'unknown').sum() for c in cat_cols],\n",
    "    'Unknown (%)': [(df[c].eq('unknown').sum() / len(df)) * 100 for c in cat_cols],\n",
    "    'Unique Values': [df[c].nunique() for c in cat_cols]\n",
    "})\n",
    "\n",
    "cat_info = cat_info.sort_values(by='Unknown Count', ascending=False)\n",
    "cat_info.style.format({'Unknown (%)': '{:.2f}%'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ae5e56",
   "metadata": {},
   "source": [
    "### Cross tablulation between categorical variable and the goal attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3307cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for predictor in cat_cols:\n",
    "    print(pd.crosstab(index=df[predictor], columns=df['y']),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f1c27c",
   "metadata": {},
   "source": [
    "### Numerical Feature Distributions\n",
    "\n",
    "To better understand the data distribution of each **numerical variable**, histograms were plotted with **Kernel Density Estimation (KDE)**.  \n",
    "This visualization helps identify skewness, outliers, and potential normalization needs before model training.\n",
    "\n",
    "The code below dynamically arranges the plots in a grid based on the total number of numerical features, ensuring a clear and organized layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e862d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "colx = 3\n",
    "numero_features = len(num_cols)\n",
    "n_rows = math.ceil(numero_features / colx)\n",
    "\n",
    "plt.figure(figsize=(16, n_rows * 3))\n",
    "\n",
    "for i, col in enumerate(num_cols, 1):\n",
    "    plt.subplot(n_rows, colx, i)\n",
    "    sns.histplot(df[col], kde=True, bins=20, color='steelblue')\n",
    "    plt.title(f\"Distribuição de {col}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a48904d",
   "metadata": {},
   "source": [
    "### Check correlation between numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a52a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns\n",
    "num_cols = df.select_dtypes(include=['int64','float64']).columns\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr = df[num_cols].corr()\n",
    "\n",
    "# Show heatmap of correlations\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation between numerical variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a951d6a1",
   "metadata": {},
   "source": [
    "### Check correlation between categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ec086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(x, y):\n",
    "    \"\"\"Compute Cramér's V statistic for categorical-categorical association.\"\"\"\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix, correction=False)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1)) / (n-1))\n",
    "    rcorr = r - ((r-1)**2) / (n-1)\n",
    "    kcorr = k - ((k-1)**2) / (n-1)\n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "cat_cols = df.select_dtypes(include='object').columns\n",
    "\n",
    "cramers_results = pd.DataFrame(index=cat_cols, columns=cat_cols, dtype=float)\n",
    "\n",
    "for c1 in cat_cols:\n",
    "    for c2 in cat_cols:\n",
    "        if c1 == c2:\n",
    "            cramers_results.loc[c1, c2] = 1.0\n",
    "        else:\n",
    "            cramers_results.loc[c1, c2] = cramers_v(df[c1], df[c2])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cramers_results, annot=True, cmap='Blues', fmt=\".2f\", square=True)\n",
    "plt.title(\"Cramér’s V Association between Categorical Variables\", fontsize=14, pad=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d0717",
   "metadata": {},
   "source": [
    "--------------------\n",
    "## Descriptive statistics\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3638b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = 'number').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739ad145",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = 'object').T "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a00dc7",
   "metadata": {},
   "source": [
    "### Class balance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faccf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "(df['y'].value_counts(normalize=True).sort_index()*100).plot(kind='bar')\n",
    "plt.title('Target distribution (percentage)')\n",
    "plt.ylabel('Percent')\n",
    "plt.xlabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76775074",
   "metadata": {},
   "source": [
    "-------------------\n",
    "# Data cleaning, feature selection\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa65533",
   "metadata": {},
   "source": [
    "### Dropping duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea677ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ea0248",
   "metadata": {},
   "source": [
    "### Dropping columns with high correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746dc8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(columns=['pdays', 'cons.price.idx', 'cons.conf.idx', 'nr.employed'], inplace=True)\n",
    "df.drop(columns=['cons.price.idx', 'cons.conf.idx', 'nr.employed'], inplace=True)\n",
    "#df.drop(columns=['pdays','nr.employed', 'euribor3m'], inplace=True)\n",
    "#df.drop(columns=['nr.employed', 'euribor3m'], inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c467e11",
   "metadata": {},
   "source": [
    "### Handling 'unknown' values and Missing Values\n",
    "\n",
    "For categorical variables, missing or `'unknown'` entries were handled to ensure data quality and model interpretability:\n",
    "\n",
    "- The variables **`education`** and **`default`** were **kept and modeled**, as the `'unknown'` category may contain predictive meaning (e.g., clients with unavailable credit history).  \n",
    "  - Missing values in these columns were filled with `'unknown'` to maintain consistency.  \n",
    "- For **`job`**, **`marital`**, **`housing`**, and **`loan`**, rows containing `'unknown'` values were **removed**, since their frequency was **very low (<3%)**, minimizing data loss while improving dataset reliability.  \n",
    "- The numeric variable **`pdays`** had values of `999` — indicating no previous contact — replaced with `-1` to make this condition explicit for modeling.  \n",
    "\n",
    "Finally, a summary table was created to display for each categorical feature:\n",
    "- Data type  \n",
    "- Number and percentage of `'unknown'` values  \n",
    "- Total unique categories  \n",
    "\n",
    "This helps evaluate the distribution of missing or undefined information across categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b486ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['housing'] != 'unknown']\n",
    "df = df[df['loan'] != 'unknown']\n",
    "df = df[df['job'] != 'unknown']\n",
    "df = df[df['marital'] != 'unknown']\n",
    "df['education'] = df['education'].fillna('unknown')\n",
    "df['default'] = df['default'].fillna('unknown')\n",
    "df['pdays'] = df['pdays'].replace(999, -1)\n",
    "\n",
    "cat_cols = df.select_dtypes(include='object').columns\n",
    "cat_info = pd.DataFrame({\n",
    "    'Data Type': df[cat_cols].dtypes,\n",
    "    'Unknown Count': [(df[c] == 'unknown').sum() for c in cat_cols],\n",
    "    'Unknown (%)': [(df[c].eq('unknown').sum() / len(df)) * 100 for c in cat_cols],\n",
    "    'Unique Values': [df[c].nunique() for c in cat_cols]\n",
    "})\n",
    "\n",
    "cat_info = cat_info.sort_values(by='Unknown Count', ascending=False)\n",
    "cat_info.style.format({'Unknown (%)': '{:.2f}%'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c33cf4",
   "metadata": {},
   "source": [
    "### Categorical variable encoding\n",
    "\n",
    "The categorical variables are converted into numeric form:\n",
    "- **`education`** and **`month`** are ordinal, so they are mapped to ordered numeric codes.  \n",
    "- Other categorical variables are nominal and are transformed using **one-hot encoding** (`pd.get_dummies()`), creating binary columns.  \n",
    "- The target **`y`** is encoded with `LabelEncoder` (`no` = 0, `yes` = 1).\n",
    "\n",
    "This ensures all variables are numeric and ready for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d46c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_order = ['illiterate', 'basic.4y', 'basic.6y', 'basic.9y',\n",
    "              'high.school', 'professional.course', 'university.degree', 'unknown']\n",
    "month_order = ['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec']\n",
    "\n",
    "df['education'] = pd.Categorical(df['education'], categories=edu_order, ordered=True).codes\n",
    "df['month'] = pd.Categorical(df['month'], categories=month_order, ordered=True).codes\n",
    "\n",
    "nominal_cols = ['job', 'marital', 'default', 'housing', 'loan',\n",
    "                'contact', 'day_of_week', 'poutcome']\n",
    "dfML = df.copy()\n",
    "dfML = pd.get_dummies(dfML, columns=nominal_cols, drop_first=True)\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['y'] = le.fit_transform(df['y'])\n",
    "dfML['y'] = le.fit_transform(dfML['y'])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29380773",
   "metadata": {},
   "source": [
    "### Scale numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09762d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_cols_scaled = ['age','campaign','pdays','previous','emp.var.rate','cons.price.idx',\n",
    "#                    'cons.conf.idx']\n",
    "# num_cols_scaled = ['age','campaign','previous','emp.var.rate','cons.price.idx',\n",
    "#                     'cons.conf.idx']\n",
    "num_cols_scaled = ['age','campaign','pdays','previous','emp.var.rate','euribor3m',]\n",
    "#num_cols_scaled = ['age','campaign','previous','emp.var.rate','euribor3m',]\n",
    "scaler = StandardScaler()\n",
    "df[num_cols_scaled] = scaler.fit_transform(df[num_cols_scaled])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0141e835",
   "metadata": {},
   "source": [
    "### Columns dropping based on the set description\n",
    "\n",
    "The column **`duration`** will be removed from the modeling dataset to avoid data leakage,  \n",
    "but will be **saved separately** for later analysis and evaluation at the end of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc69588",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = df['duration'].copy()\n",
    "df=df.drop(columns=['duration'])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd11e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr(numeric_only=True)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(corr, cmap='coolwarm', annot=False)\n",
    "plt.title(\"Matriz de Correlação\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5ae7f6",
   "metadata": {},
   "source": [
    "### Correlation between Features and Target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc2340",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.drop(columns=['y']).copy()\n",
    "df_temp['y'] = df['y']\n",
    "\n",
    "corr_with_target = df_temp.corr(numeric_only=True)['y'].sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(6,10))\n",
    "sns.heatmap(corr_with_target.to_frame(), annot=True, cmap=\"coolwarm\", cbar=False)\n",
    "plt.title(\"Correlation between Features and Target (y)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce095c",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d5d157",
   "metadata": {},
   "source": [
    "### Train/Test Split and Cross-Validation\n",
    "\n",
    "Experiment with different train/test splits (e.g., 70/30, 80/20, 90/10) to assess model robustness.\n",
    "For a more reliable evaluation, use Stratified K-Fold Cross-Validation, which averages results across multiple partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a2d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['y'])\n",
    "y = df['y']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "train_df = X_train.copy()\n",
    "train_df['y'] = y_train\n",
    "\n",
    "test_df = X_test.copy()\n",
    "test_df['y'] = y_test\n",
    "\n",
    "\n",
    "X = dfML.drop(columns=['y'])\n",
    "y = dfML['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "train_dfML = X_train.copy()\n",
    "train_dfML['y'] = y_train\n",
    "\n",
    "test_dfML = X_test.copy()\n",
    "test_dfML['y'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9888bb5",
   "metadata": {},
   "source": [
    "### Applying SMOTE to Balance the Training Data\n",
    "\n",
    "Use the Synthetic Minority Over-sampling Technique (SMOTE) to generate synthetic samples of the minority class, ensuring a balanced training dataset and improving model fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c03ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_dfML.drop(columns=['y']).copy()\n",
    "y_train = train_dfML['y']\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nOriginal train shape:\", X_train.shape)\n",
    "print(\"Resampled train shape:\", X_train_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb67942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "rfe = RFE(model, n_features_to_select=10)\n",
    "rfe.fit(X, y)\n",
    "print(\"Features selected:\", X.columns[rfe.support_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94106795",
   "metadata": {},
   "source": [
    "### Interpretation of Feature Selection Methods\n",
    "\n",
    "Three feature selection techniques were applied:\n",
    "- **Chi-Square Test** for categorical variables,\n",
    "- **ANOVA Test** for continuous variables, and\n",
    "- **LASSO Regularization** for multivariate feature importance.\n",
    "\n",
    "These methods confirmed which features have a statistically significant relationship with the target variable (`y`).\n",
    "The goal was not only to reduce dimensionality, but also to validate the relevance and predictive strength of each feature.\n",
    "\n",
    "A reduced dataset (**dfFS**) was then created containing only the significant variables.\n",
    "This dataset was compared with the full model (**dfML**) to evaluate whether model simplicity\n",
    "could be achieved without compromising predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FunctionChisq(inpData, TargetVariable, CategoricalVariablesList):\n",
    "\n",
    "\n",
    "    FiltPredictors = []\n",
    "\n",
    "    for predictor in CategoricalVariablesList:\n",
    "        CrossTabResult = pd.crosstab(index=inpData[TargetVariable],\n",
    "                                     columns=inpData[predictor])\n",
    "        ChiSqResult = chi2_contingency(CrossTabResult)\n",
    "        p_value = ChiSqResult[1]\n",
    "\n",
    "        if p_value < 0.05:\n",
    "            print(f\"{predictor} IS correlated with {TargetVariable} | P-Value: {p_value:.4f}\")\n",
    "        else:\n",
    "            print(f\"{predictor} is NOT correlated with {TargetVariable} | P-Value: {p_value:.4f}\")\n",
    "            FiltPredictors.append(predictor)\n",
    "            \n",
    "    return FiltPredictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fba639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_vars = ['job', 'marital', 'education', 'default', 'housing',\n",
    "                    'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n",
    "\n",
    "insignificant_vars = FunctionChisq(inpData=train_df, TargetVariable='y', CategoricalVariablesList=categorical_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6114e71",
   "metadata": {},
   "source": [
    "### Continuous vs Categorical using ANOVA test\n",
    "\n",
    "- **Assumption (H₀):** There is **no relationship** between the continuous predictor and the target variable.  \n",
    "  In other words, the mean of the numeric variable is **the same across both classes** of the target (`y`).\n",
    "\n",
    "- **Alternative (H₁):** There **is a relationship**, meaning that the mean values differ significantly between the groups of `y`.\n",
    "\n",
    "The ANOVA test evaluates the probability that the null hypothesis (H₀) is true.  \n",
    "If the **p-value < 0.05**, we reject H₀ and conclude that the variable is **significantly correlated** with the target.  \n",
    "If the **p-value ≥ 0.05**, we fail to reject H₀, meaning there is **no significant difference** between the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed10f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FunctionAnova(inpData, TargetVariable, ContinuousPredictorList):\n",
    "    FiltPredictors = []\n",
    "\n",
    "    print('##### ANOVA Results #####\\n')\n",
    "    for predictor in ContinuousPredictorList:\n",
    "        try:\n",
    "            CategoryGroupLists = inpData.groupby(TargetVariable)[predictor].apply(list)\n",
    "            AnovaResults = f_oneway(*CategoryGroupLists)\n",
    "\n",
    "            p_value = AnovaResults[1]\n",
    "\n",
    "            if p_value < 0.05:\n",
    "                print(f\"{predictor} IS correlated with {TargetVariable} | P-Value: {p_value:.4f}\")\n",
    "            else:\n",
    "                print(f\"{predictor} is NOT correlated with {TargetVariable} | P-Value: {p_value:.4f}\")\n",
    "                FiltPredictors.append(predictor)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not test {predictor}: {e}\")\n",
    "\n",
    "    return FiltPredictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a2ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous_vars = [\n",
    "#     'age', 'campaign', 'pdays','previous', 'emp.var.rate',\n",
    "#     'cons.price.idx', 'cons.conf.idx'\n",
    "# ]\n",
    "# continuous_vars = [\n",
    "#     'age', 'campaign', 'previous', 'emp.var.rate',\n",
    "#     'cons.price.idx', 'cons.conf.idx'\n",
    "# ]\n",
    "continuous_vars = [\n",
    "    'age', 'campaign','pdays', 'previous','euribor3m', 'emp.var.rate',\n",
    "]\n",
    "# continuous_vars = [\n",
    "#     'age', 'campaign', 'previous','euribor3m', 'emp.var.rate',\n",
    "# ]\n",
    "insignificant_continuous_vars = FunctionAnova(inpData=train_df, TargetVariable='y', ContinuousPredictorList=continuous_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164059a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def lasso_regularization(df):\n",
    "\n",
    "    X = df.iloc[:,:-1].copy()          \n",
    "    y = df.iloc[:,-1].copy() \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    sel_ = SelectFromModel(LogisticRegression(C=0.5, penalty='l1', solver='liblinear', random_state=10))\n",
    "\n",
    "    sel_.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "    selected_feat = X_train.columns[(sel_.get_support())]\n",
    "    \n",
    "    print(\"Number of features which coefficient was shrank to zero: \", np.sum(sel_.estimator_.coef_ == 0))\n",
    "    \n",
    "    removed_feats = X_train.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]\n",
    "    print('Removed features by Lasso: ',removed_feats) \n",
    "\n",
    "    return X_train.columns[(sel_.estimator_.coef_ != 0).ravel().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = lasso_regularization(train_dfML)\n",
    "print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06cfb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_categorical = [var for var in categorical_vars if var not in insignificant_vars]\n",
    "\n",
    "selected_continuous = [var for var in continuous_vars if var not in insignificant_continuous_vars]\n",
    "\n",
    "selected_all = list(set(list(selected_categorical) + \n",
    "                        list(selected_continuous) + \n",
    "                        list(selected_features)))\n",
    "\n",
    "print(f\"Total features selected: {len(selected_all)}\")\n",
    "print(\"Selected variables:\\n\", selected_all)\n",
    "dfFS_cols = []\n",
    "\n",
    "for col in selected_all:\n",
    "    matched = [c for c in dfML.columns if col in c]\n",
    "    dfFS_cols.extend(matched)\n",
    "\n",
    "dfFS_cols = list(set(dfFS_cols + ['y']))\n",
    "\n",
    "dfFS = dfML[dfFS_cols].copy()\n",
    "\n",
    "print(f\"\\ndfFS created successfully with {len(dfFS.columns)-1} features.\")\n",
    "print(\"Shape:\", dfFS.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e5c3e",
   "metadata": {},
   "source": [
    "-------------------\n",
    "# Modeling and tunning\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1015a3",
   "metadata": {},
   "source": [
    "### Model Evaluation Function\n",
    "\n",
    "This function evaluates a classifier’s performance across multiple **classification thresholds** (`0.3`, `0.5`, and `0.7`), providing a detailed analysis of how sensitivity and precision change with decision boundaries.\n",
    "\n",
    "For each model:\n",
    "- The classifier is trained using the training set.  \n",
    "- Predicted probabilities (`predict_proba`) are used to compute **ROC-AUC** and generate the **ROC Curve**, visualizing the trade-off between the True Positive Rate and False Positive Rate.  \n",
    "- For each threshold value:\n",
    "  - Predicted classes are derived from probabilities (`y_proba >= threshold`).\n",
    "  - **Precision**, **Recall**, **F1-score**, **Balanced Accuracy**, and **Matthews Correlation Coefficient (MCC)** are computed.  \n",
    "  - A **confusion matrix** is plotted to visualize prediction outcomes (TP, FP, FN, TN).  \n",
    "\n",
    "This unified evaluation approach enables a consistent and comprehensive comparison across multiple classification models, ensuring fair assessment of both overall performance (AUC) and class-level behavior under different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a210279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test, thresholds=[0.3, 0.5, 0.7]):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    auc_value = roc_auc_score(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, label=f\"{model.__class__.__name__} (AUC={auc_value:.3f})\")\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    for thr in thresholds:\n",
    "        print(f\"\\n--- Threshold = {thr} ---\")\n",
    "        y_pred = (y_proba >= thr).astype(int)\n",
    "        \n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"Balanced Accuracy:\", balanced_accuracy_score(y_test, y_pred))\n",
    "        print(\"Matthews Corr. Coefficient (MCC):\", matthews_corrcoef(y_test, y_pred))\n",
    "        print(\"AUC:\", auc_value)\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(cm)\n",
    "        disp.plot(cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix (Threshold = {thr})')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b948beb",
   "metadata": {},
   "source": [
    "-------------------\n",
    "# Model Training and Evaluation\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c0e52a",
   "metadata": {},
   "source": [
    "Train and evaluate multiple machine learning models — **Random Forest**, **Logistic Regression**, **Decision Tree**, **K-Nearest Neighbours (KNN)**, **Naïve Bayes**, **XGBoost**, **LightGBM**, and **Multi-Layer Perceptron (MLP Neural Network)** — using the same balanced training dataset.\n",
    "\n",
    "Each model is assessed based on **cross-validation ROC-AUC performance** and **test-set evaluation metrics**, including:\n",
    "\n",
    "- **Precision**\n",
    "- **Recall**\n",
    "- **F1-Score**\n",
    "- **Balanced Accuracy**\n",
    "- **ROC-AUC**\n",
    "- **Matthews Correlation Coefficient (MCC)**\n",
    "\n",
    "The results are compared to identify which classifier provides the best trade-off between **sensitivity and specificity**, as well as overall **robustness** on the minority class.\n",
    "\n",
    "Ensemble and deep learning models such as **XGBoost**, **LightGBM**, and **MLP** are further validated using **5-fold Stratified Cross-Validation** to ensure **consistency** and **generalisation** across folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b50e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- RANDOM FOREST ---\")\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "scores = cross_val_score(rf_model, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "\n",
    "print(\"ROC-AUC médio (5-fold CV):\", round(scores.mean(), 4))\n",
    "print(\"Desvio padrão:\", round(scores.std(), 4))\n",
    "\n",
    "\n",
    "print(\"\\n--- LOGISTIC REGRESSION ---\")\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "evaluate_model(lr_model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\n--- DECISION TREE ---\")\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    criterion='gini', \n",
    "    max_depth=None, \n",
    "    min_samples_split=2, \n",
    "    random_state=42\n",
    ")\n",
    "evaluate_model(dt_model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\n--- K-NEAREST NEIGHBOURS (KNN) ---\")\n",
    "knn_model = KNeighborsClassifier(\n",
    "    n_neighbors=5,  \n",
    "    metric='minkowski', \n",
    "    p=2             \n",
    ")\n",
    "evaluate_model(knn_model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\n--- NAIVE BAYES ---\")\n",
    "nb_model = GaussianNB()\n",
    "evaluate_model(nb_model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\n--- XGBOOST ---\")\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "evaluate_model(xgb_model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\n--- LIGHTGBM ---\")\n",
    "lgbm_model = LGBMClassifier(\n",
    "    n_estimators=700,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "evaluate_model(lgbm_model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\n--- MLP (Neural Network) ---\")\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    learning_rate_init=1e-3,\n",
    "    alpha=1e-4,              \n",
    "    batch_size=256,\n",
    "    max_iter=100,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "evaluate_model(mlp_model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, mdl in [\n",
    "    (\"XGB\", xgb_model),\n",
    "    (\"LGBM\", lgbm_model),\n",
    "    (\"MLP\", mlp_model)\n",
    "]:\n",
    "    scores = cross_val_score(mdl, X_train, y_train, cv=cv, scoring=\"roc_auc\", n_jobs=-1)\n",
    "    print(f\"{name} | ROC-AUC CV: {scores.mean():.4f} ± {scores.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14183ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- LOGISTIC REGRESSION ---\")\n",
    "evaluate_model(LogisticRegression(max_iter=1000),X_train_res, y_train_res, X_test, y_test)\n",
    "\n",
    "\n",
    "print(\"\\n--- DECISION TREE ---\")\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    criterion='gini', \n",
    "    max_depth=None, \n",
    "    min_samples_split=2, \n",
    "    random_state=42,\n",
    ")\n",
    "evaluate_model(dt_model, X_train_res, y_train_res, X_test, y_test)\n",
    "\n",
    "print(\"\\n--- K-NEAREST NEIGHBOURS (KNN) ---\")\n",
    "knn_model = KNeighborsClassifier(\n",
    "    n_neighbors=5,  \n",
    "    metric='minkowski', \n",
    "    p=2             \n",
    ")\n",
    "evaluate_model(knn_model, X_train_res, y_train_res, X_test, y_test)\n",
    "\n",
    "print(\"\\n--- NAIVE BAYES ---\")\n",
    "nb_model = GaussianNB()\n",
    "evaluate_model(nb_model, X_train_res, y_train_res, X_test, y_test)\n",
    "\n",
    "print(\"\\n--- XGBOOST ---\")\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "evaluate_model(xgb_model, X_train_res, y_train_res, X_test, y_test)\n",
    "\n",
    "print(\"\\n--- LIGHTGBM ---\")\n",
    "lgbm_model = LGBMClassifier(\n",
    "    n_estimators=700,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "evaluate_model(lgbm_model, X_train_res, y_train_res, X_test, y_test)\n",
    "\n",
    "print(\"\\n--- MLP (Neural Network) ---\")\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    learning_rate_init=1e-3,\n",
    "    alpha=1e-4,               \n",
    "    batch_size=256,\n",
    "    max_iter=100,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "evaluate_model(mlp_model, X_train_res, y_train_res, X_test, y_test)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, mdl in [\n",
    "    (\"XGB\", xgb_model),\n",
    "    (\"LGBM\", lgbm_model),\n",
    "    (\"MLP\", mlp_model)\n",
    "]:\n",
    "    scores = cross_val_score(mdl, X_train_res, y_train_res, cv=cv, scoring=\"roc_auc\", n_jobs=-1)\n",
    "    print(f\"{name} | ROC-AUC CV: {scores.mean():.4f} ± {scores.std():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
